---
title: The end of manual audits
date: '2025-10-02'
---

The advent of LLM reasoning models has opened up doors to create systems that mimic human knowledge work. Coding, science, market research, and many more.

We believe smart contract auditing is one of them.

There are many tools (static analysis, fuzzing, etc) which assist human auditors.

But they only augment the human guided process of auditing. These tools catch specific classes of bugs but fail to reason holistically about protocol-level invariants, which is something reasoning models excel at.

We believe LLM reasoning models have the potential to surpass human auditors in reliability

A lot of R&D is required, but at some point we will trust AI auditors more than human auditors. As we are starting to trust self-driving cars more than we trust cars driven by humans.

### How to build an AI auditor?

One of the most important components in building highly capable AI systems is building robust [evals](https://www.theaisignal.com/p/ai-evals-what-they-are-why-they-matter). We must create evals from real-world audit reports ([code4rena](https://code4rena.com/), etc) with real-world vulnerabilities.

And in order to make the AI system trustworthy, it must be able to uncover nearly 100% of the vulnerabilities in the evals.

It’ll require a lot of prompt engineering, context engineering, and possibly reinforcement learning as well.

### Beyond smart contract auditing

Smart contract auditing is a good starting point since there are a lot of data available to build and test AI systems.

In order to be as good, or better than human auditors, the AI has to be general enough to know how to think in first principles, and approach auditing in a way that’s just not “pattern matching”.

It has to be able to identify bugs that it has never seen before.

Once there is an AI system that’s capable of doing that, it can be expanded into systems beyond smart contracts.
